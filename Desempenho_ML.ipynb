{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Desempenho-ML",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOnJAxIM01UgebkAWMwG44f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GenoaroSJr/DataScienceSmarkio/blob/main/Desempenho_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toPzv96AGcFK"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUfxw2OoGcWJ"
      },
      "source": [
        "from google.colab import drive\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "6X-uy2tzGLsR",
        "outputId": "7fb25150-5690-47f9-b70e-8728bf5c9b77"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Smarkio/df_tratado.csv\")\r\n",
        "df_original = pd.read_excel(\"/content/drive/MyDrive/teste_smarkio_Lbs.xls\")\r\n",
        "df = df.drop(columns=['Unnamed: 0'])\r\n",
        "df.head()"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pred_class</th>\n",
              "      <th>probabilidade</th>\n",
              "      <th>status</th>\n",
              "      <th>True_class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.079892</td>\n",
              "      <td>approved</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0.379377</td>\n",
              "      <td>approved</td>\n",
              "      <td>74.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0.379377</td>\n",
              "      <td>approved</td>\n",
              "      <td>74.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0.420930</td>\n",
              "      <td>approved</td>\n",
              "      <td>74.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0.607437</td>\n",
              "      <td>approved</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Pred_class  probabilidade    status  True_class\n",
              "0           0       0.079892  approved         0.0\n",
              "1           0       0.379377  approved        74.0\n",
              "2           0       0.379377  approved        74.0\n",
              "3           0       0.420930  approved        74.0\n",
              "4           1       0.607437  approved         2.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HK36T2hLBeR",
        "outputId": "e42c640a-ac8a-43e4-b98d-e9f567cd9a71"
      },
      "source": [
        "x = len(df_original)\r\n",
        "\r\n",
        "Truee = df_original['True_class']\r\n",
        "Pred = df_original['Pred_class']\r\n",
        "\r\n",
        "for i in range(x):\r\n",
        "  if (Truee[i] >= 0):\r\n",
        "    ...\r\n",
        "  else:\r\n",
        "    Truee[i] = Pred[i]\r\n",
        "print(df_original)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     Pred_class  probabilidade    status  True_class\n",
            "0             2       0.079892  approved         0.0\n",
            "1             2       0.379377  approved        74.0\n",
            "2             2       0.379377  approved        74.0\n",
            "3             2       0.420930  approved        74.0\n",
            "4             2       0.607437  approved         2.0\n",
            "..          ...            ...       ...         ...\n",
            "595          74       0.432421  approved        74.0\n",
            "596          82       0.590576  approved        82.0\n",
            "597          92       0.915543  approved        92.0\n",
            "598          96       0.334495  approved        96.0\n",
            "599          99       0.373226  approved        22.0\n",
            "\n",
            "[600 rows x 4 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9QS-AHnGY8S"
      },
      "source": [
        "pred_class = df['Pred_class']"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gi4Oc63YuQg0",
        "outputId": "cf9049bf-a56d-4079-fc15-5e4c640c5a7b"
      },
      "source": [
        "pred_class = np.array(pred_class)\r\n",
        "pred_class = list(pred_class)\r\n",
        "pred_class.count(0)\r\n",
        "print(\"Errados:\", pred_class.count(0), \"Certos: \", pred_class.count(1))\r\n",
        "print(\"soma: \", pred_class.count(0)+pred_class.count(1))"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Errados: 181 Certos:  419\n",
            "soma:  600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MT24EPf4P3h_"
      },
      "source": [
        "### Accuracy\r\n",
        "\r\n",
        "Accuracy ou acuracia, diz quantas classes foram classificadas corretamente. Como já foi demonstrado acima, de 600 dados, 181 foram classificados como errados e 419 foram classificados como certos. \r\n",
        "\r\n",
        "Mesmo sendo uma métrica de fácil uso e interpretação, a acuracia não é uma métrica confiavel, pois:\r\n",
        "  - Um valor alto não significa uma boa permformace do algorítimo.\r\n",
        "  - Ela considera o mesmo peso para todos os erros.\r\n",
        "\r\n",
        "No geral, essa métrica deve ser usada quando há a mesma proporção de exemplos para cada classe e quando a penalidade de acerto e erro para cada classe forem as mesmas.\r\n",
        "\r\n",
        "Como o dataset disponibilizado tem uma boa variedade de dados (181 erros e 419 acertos), é valido o uso."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRaBdlDvNBxK",
        "outputId": "9ea02215-3ce7-4d42-fd58-fd3d897ab12b"
      },
      "source": [
        "#Accuracy\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "accuracy = accuracy_score(df_original['True_class'], df_original['Pred_class'])\r\n",
        "accuracy *= 100\r\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 69.83333333333334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCZC05X3QFry"
      },
      "source": [
        "### Precisão\r\n",
        "\r\n",
        "Esta métrica é definida pela razão entre a quantidade de exemplos classificados corretamente como positivos e o total de exemplos classificados como positivos. É a métrica que \"responde\" o questionamento \"dos exemplos classificados como positivos, quantos realmente são?\"\r\n",
        "\r\n",
        "É uma métrica utilizada quando se quer identificar um determinado resultado de forma mais precisa, no caso, minizar os falses positivos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztb_HKFZNMmx",
        "outputId": "7da3b473-c520-467d-acc4-33869b902087"
      },
      "source": [
        "from sklearn.metrics import  precision_score\r\n",
        "precision = precision_score(df_original['True_class'], df_original['Pred_class'], average='weighted', zero_division=0)\r\n",
        "precision *= 100\r\n",
        "print(\"Precision:\",precision)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision: 69.88175116236557\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGvMhOwfUcGg"
      },
      "source": [
        "### F1 Score\r\n",
        "\r\n",
        "O F1-Score é uma métrica que leva em consideração outras duas métrias, a precisão e a revocação. A F1-Score será alta se ambas as suas componentes também o forem. Portanto, tendo esse parâmetro alto, fica claro que o modelo é capaz de acertas suas predições como de recuperar os exemplos da classe de interesse. No geral, quanto maior o F1-Score, melhor.\r\n",
        "\r\n",
        "Caso o F1-Score esteja baixo será é necessário avaliar a Precisão e o Recall. O valor baixo indica que há um problema na geração de falsos positivos ou negativos. \r\n",
        "\r\n",
        "Ela é boa quando o dataset possui classes desproporcionais, e o modelo não emite probabilidade. No geral, é uma métrica que substitui bem a Acuracia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqgFbLMwQ45v",
        "outputId": "ac2f294a-24f0-4875-81af-ecf1311439d7"
      },
      "source": [
        "from sklearn.metrics import f1_score\r\n",
        "f1_score = f1_score(df_original['True_class'], df_original['Pred_class'], average='weighted', zero_division=0)\r\n",
        "f1_score *= 100\r\n",
        "print(f1_score)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "68.31017455927221\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KUDefhCVH-x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}